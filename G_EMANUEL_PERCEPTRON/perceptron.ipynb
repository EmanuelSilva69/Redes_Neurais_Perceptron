{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Atividade de Redes Neurais de Emanuel Lopes Silva"
      ],
      "metadata": {
        "id": "co0N8ZRrkDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este notebook foi desenvolvido como um guia te√≥rico e pr√°tico introdut√≥rio sobre Redes Neurais Artificiais (RNAs), com o objetivo de apresentar os conceitos fundamentais e exemplos de implementa√ß√£o em Python de forma clara e acess√≠vel.\n",
        "\n",
        "As Redes Neurais Artificiais s√£o sistemas computacionais inspirados na estrutura e no funcionamento do c√©rebro humano. Elas utilizam unidades chamadas neur√¥nios artificiais, organizadas em camadas e interconectadas por pesos sin√°pticos, os quais s√£o ajustados automaticamente durante o aprendizado. Com isso, as RNAs s√£o capazes de identificar padr√µes, realizar classifica√ß√µes e fazer previs√µes com base em dados, sendo amplamente utilizadas em diversas aplica√ß√µes da intelig√™ncia artificial."
      ],
      "metadata": {
        "id": "Vao65IKjmWYc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#1. Introdu√ß√£o ao modelo de Perceptron\n",
        "\n"
      ],
      "metadata": {
        "id": "iFJOlLgLixWJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.1. O que √© o Perceptron?\n",
        "\n",
        "###1.1.1 Defini√ß√£o e prop√≥sito\n",
        "\n",
        "O Perceptron √© considerado o primeiro modelo computacional de rede neural com capacidade de aprendizado supervisionado, sendo formulado por Frank Rosenblatt, em 1958. Seu objetivo era simular o processo de reconhecimento de padr√µes por organismos biol√≥gicos, estabelecendo uma liga√ß√£o entre a estrutura neural e o comportamento adaptativo. Segundo Rosenblatt, o Perceptron √© um \"modelo probabil√≠stico de organiza√ß√£o e armazenamento de informa√ß√£o no c√©rebro\", cuja fun√ß√£o central √© aprender a associar padr√µes de entrada a respostas esperadas com base em exemplos fornecidos previamente (ROSENBLATT, 1958).\n",
        "\n",
        "Seu prop√≥sito pode ser simplificado nos seguintes pontos:\n",
        "- Simular o processo de **aprendizado supervisionado** inspirado no c√©rebro humano.\n",
        "- Estabelecer um **modelo matem√°tico** capaz de reconhecer padr√µes a partir de exemplos.\n",
        "- Demonstrar que √© poss√≠vel implementar **fun√ß√µes de decis√£o lineares** com neur√¥nios artificiais simples.\n",
        "- Investigar como a **informa√ß√£o √© armazenada e organizada** em redes de conex√µes sin√°pticas.\n",
        "- Propor um sistema que aprende ajustando **pesos sin√°pticos** com base no erro da resposta.\n",
        "- Oferecer uma base te√≥rica para a constru√ß√£o de redes neurais mais complexas no futuro.\n",
        "- Explorar o potencial de redes com **unidades simples interconectadas** para gerar comportamento inteligente.\n",
        "\n",
        "###1.1.2 Origem Hist√≥rica\n",
        "\n",
        "A origem do modelo de Perceptron remonta √† tentativa de compreender os mecanismos de aprendizagem e reconhecimento presentes nos sistemas nervosos biol√≥gicos. O trabalho seminal de McCulloch e Pitts (1943) j√° propunha uma estrutura l√≥gica para descrever a atividade neural por meio de fun√ß√µes booleanas, estabelecendo os fundamentos do que viria a ser conhecido como redes neurais artificiais. Eles demonstraram que \"a atividade de qualquer neur√¥nio pode ser representada como uma proposi√ß√£o\" e que redes neurais poderiam, em princ√≠pio, implementar qualquer fun√ß√£o l√≥gica comput√°vel com base em conex√µes sin√°pticas e limiares de ativa√ß√£o‚Äã.\n",
        "\n",
        "D√©cadas depois, Frank Rosenblatt, em seu artigo de 1958, formalizou o modelo do Perceptron como uma m√°quina probabil√≠stica capaz de realizar tarefas de classifica√ß√£o por meio da aprendizagem dos pesos sin√°pticos. Segundo Rosenblatt, o Perceptron representa um \"sistema nervoso hipot√©tico\" que armazena informa√ß√£o por meio de conex√µes modific√°veis, sendo capaz de generalizar e reconhecer padr√µes com base em est√≠mulos sensoriais‚Äã\n",
        ". Esse modelo √© considerado a primeira implementa√ß√£o pr√°tica de uma rede neural com capacidade de aprendizado supervisionado.\n",
        "\n",
        "O entusiasmo inicial com o Perceptron, no entanto, foi confrontado com cr√≠ticas importantes. O livro Perceptrons de Minsky e Papert (1969) demonstrou limita√ß√µes fundamentais do modelo, especialmente sua incapacidade de resolver problemas n√£o linearmente separ√°veis, como o XOR. Essa cr√≠tica desacelerou o avan√ßo das redes neurais por quase duas d√©cadas.\n",
        "\n",
        "Somente nos anos 1980, com o trabalho de Rumelhart, Hinton e McClelland, foi poss√≠vel revitalizar o campo por meio do conceito de Parallel Distributed Processing (PDP). Esses pesquisadores defenderam que o processamento inteligente poderia emergir da intera√ß√£o paralela de unidades simples de processamento, refor√ßando o papel das redes distribu√≠das e do aprendizado supervisionado por propaga√ß√£o do erro como alternativa vi√°vel e biologicamente plaus√≠vel para modelar a cogni√ß√£o humana‚Äã\n",
        ".\n",
        "\n",
        "###1.1.3 Import√¢ncia na evolu√ß√£o das redes neurais\n",
        "O modelo do Perceptron teve um papel fundamental na hist√≥ria da Intelig√™ncia Artificial, sendo considerado o ponto de partida das redes neurais artificiais com capacidade de aprendizado. Proposto por Frank Rosenblatt em 1958, o Perceptron foi o primeiro sistema computacional que demonstrou ser poss√≠vel ensinar uma m√°quina a reconhecer padr√µes por meio de ajustes iterativos em seus par√¢metros internos, simulando o processo de aprendizagem observado em sistemas biol√≥gicos (ROSENBLATT, 1958). Seu funcionamento baseado na atualiza√ß√£o de pesos sin√°pticos a partir do erro da sa√≠da contribuiu para consolidar o paradigma conexionista como alternativa aos modelos simb√≥licos dominantes na √©poca.\n",
        "\n",
        "Ele foi essencial na evolu√ß√£o das redes neurais, pois:\n",
        "\n",
        "- Foi o primeiro modelo de rede neural com capacidade de aprendizado supervisionado.\n",
        "- Estabeleceu a ideia de ajuste de pesos sin√°pticos como mecanismo de aprendizagem.\n",
        "- Inspirou o desenvolvimento de redes neurais multicamadas (MLPs) e o uso da retropropaga√ß√£o.\n",
        "- Sua limita√ß√£o com problemas como XOR motivou pesquisas sobre redes com maior capacidade expressiva.\n",
        "- Fundamentou a transi√ß√£o de modelos simb√≥licos para modelos conexionistas na IA.\n",
        "- Tornou-se refer√™ncia obrigat√≥ria em estudos sobre o surgimento das redes neurais profundas.\n",
        "\n",
        "##1.2. Inspira√ß√£o Biol√≥gica\n",
        "###1.2.1 Compara√ß√£o entre neur√¥nios biol√≥gicos e artificiais\n",
        "\n",
        "###1.2.2 Elementos equivalentes: dendritos, corpo celular, ax√¥nio, sinapse\n",
        "\n",
        "##1.3. Estrutura de um Perceptron\n",
        "Entradas e pesos\n",
        "\n",
        "Fun√ß√£o de ativa√ß√£o (limiar)\n",
        "\n",
        "Sa√≠da bin√°ria (0 ou 1)\n",
        "\n",
        "Esquema visual do neur√¥nio Perceptron\n",
        "\n",
        "##1.4. Funcionamento B√°sico\n",
        "Etapas do processo:\n",
        "\n",
        "Soma ponderada\n",
        "\n",
        "Aplica√ß√£o da fun√ß√£o de ativa√ß√£o\n",
        "\n",
        "Gera√ß√£o da sa√≠da\n",
        "\n",
        "##1.5. F√≥rmulas Matem√°ticas\n",
        "Equa√ß√£o da sa√≠da do Perceptron:\n",
        "\n",
        "ùë¶\n",
        "=\n",
        "ùëì\n",
        "(\n",
        "‚àë\n",
        "ùëñ\n",
        "=\n",
        "1\n",
        "ùëõ\n",
        "ùë§\n",
        "ùëñ\n",
        "ùë•\n",
        "ùëñ\n",
        "+\n",
        "ùëè\n",
        ")\n",
        "y=f(\n",
        "i=1\n",
        "‚àë\n",
        "n\n",
        "‚Äã\n",
        " w\n",
        "i\n",
        "‚Äã\n",
        " x\n",
        "i\n",
        "‚Äã\n",
        " +b)\n",
        "Explica√ß√£o de cada termo:\n",
        "\n",
        "ùë•\n",
        "ùëñ\n",
        "x\n",
        "i\n",
        "‚Äã\n",
        " : entrada\n",
        "\n",
        "ùë§\n",
        "ùëñ\n",
        "w\n",
        "i\n",
        "‚Äã\n",
        " : peso\n",
        "\n",
        "ùëè\n",
        "b: vi√©s (bias)\n",
        "\n",
        "ùëì\n",
        "f: fun√ß√£o de ativa√ß√£o (degrau de Heaviside)\n",
        "\n",
        "##1.6. Algoritmo de Aprendizado do Perceptron\n",
        "L√≥gica do treinamento supervisionado\n",
        "\n",
        "Atualiza√ß√£o dos pesos:\n",
        "\n",
        "ùë§\n",
        "ùëñ\n",
        "‚Üê\n",
        "ùë§\n",
        "ùëñ\n",
        "+\n",
        "ùúÇ\n",
        "‚ãÖ\n",
        "(\n",
        "ùë¶\n",
        "real\n",
        "‚àí\n",
        "ùë¶\n",
        "previsto\n",
        ")\n",
        "‚ãÖ\n",
        "ùë•\n",
        "ùëñ\n",
        "w\n",
        "i\n",
        "‚Äã\n",
        " ‚Üêw\n",
        "i\n",
        "‚Äã\n",
        " +Œ∑‚ãÖ(y\n",
        "real\n",
        "‚Äã\n",
        " ‚àíy\n",
        "previsto\n",
        "‚Äã\n",
        " )‚ãÖx\n",
        "i\n",
        "‚Äã\n",
        "\n",
        "Papel da taxa de aprendizado\n",
        "ùúÇ\n",
        "Œ∑\n",
        "\n",
        "##1.7. Converg√™ncia e Limita√ß√µes\n",
        "Converg√™ncia garantida para problemas linearmente separ√°veis\n",
        "\n",
        "Limita√ß√µes do modelo simples (n√£o resolve o XOR)\n",
        "\n",
        "Introdu√ß√£o √† motiva√ß√£o para redes multicamadas\n",
        "\n",
        "##1.8. Visualiza√ß√µes e Intui√ß√µes Geom√©tricas\n",
        "Fronteiras de decis√£o lineares\n",
        "\n",
        "Separa√ß√£o de classes no plano cartesiano\n",
        "\n",
        "##1.9. Varia√ß√µes e Extens√µes\n",
        "Perceptron com m√∫ltiplos neur√¥nios (camada de sa√≠da com mais classes)\n",
        "\n",
        "Fun√ß√µes de ativa√ß√£o alternativas (ReLU, Sigmoid ‚Äì apenas para contextualizar)\n",
        "\n"
      ],
      "metadata": {
        "id": "I1iWiR1NvqQ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Aplica√ß√£o do Perceptron aos problemas l√≥gicos OR e AND\n"
      ],
      "metadata": {
        "id": "BYlOWqBlj4A0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.1. Revis√£o dos Problemas L√≥gicos\n",
        "Tabelas verdade dos operadores l√≥gicos AND e OR\n",
        "\n",
        "Representa√ß√£o como problemas de classifica√ß√£o bin√°ria\n",
        "\n",
        "2.2. Constru√ß√£o Manual dos Datasets\n",
        "Conjunto de dados com entrada bin√°ria (0 ou 1)\n",
        "\n",
        "Prepara√ß√£o dos pares (entrada, sa√≠da esperada)\n",
        "\n",
        "2.3. Implementa√ß√£o do Perceptron\n",
        "C√≥digo Python manual, com coment√°rios linha a linha\n",
        "\n",
        "Inicializa√ß√£o de pesos e vi√©s\n",
        "\n",
        "Fun√ß√£o de ativa√ß√£o (degrau)\n",
        "\n",
        "La√ßo de treinamento com atualiza√ß√£o dos pesos\n",
        "\n",
        "2.4. Processo de Treinamento\n",
        "N√∫mero de √©pocas\n",
        "\n",
        "C√°lculo do erro\n",
        "\n",
        "Atualiza√ß√£o iterativa dos pesos\n",
        "\n",
        "2.5. Avalia√ß√£o do Modelo\n",
        "Previs√£o de sa√≠da para cada entrada\n",
        "\n",
        "Compara√ß√£o com a sa√≠da esperada\n",
        "\n",
        "Impress√£o de resultados\n",
        "\n",
        "2.6. Visualiza√ß√£o dos Resultados\n",
        "Gr√°ficos de dispers√£o dos dados\n",
        "\n",
        "Exibi√ß√£o da fronteira de decis√£o\n",
        "\n",
        "An√°lise visual: separabilidade linear\n",
        "\n",
        "2.7. An√°lise da Fronteira de Decis√£o\n",
        "Interpreta√ß√£o geom√©trica dos pesos como coeficientes da reta\n",
        "\n",
        "Como a reta se ajusta para classificar corretamente os dados\n",
        "\n",
        "Demonstra√ß√£o com matplotlib (sem bibliotecas de IA)\n",
        "\n",
        "2.8. Discuss√£o dos Resultados\n",
        "Por que o Perceptron funciona para OR e AND?\n",
        "\n",
        "Influ√™ncia da taxa de aprendizado e n√∫mero de √©pocas\n",
        "\n",
        "Limita√ß√µes do modelo simples mesmo em problemas lineares (por ex., quando dados ruidosos s√£o inseridos)"
      ],
      "metadata": {
        "id": "FOpC_xcOzAnA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Discuss√£o do problema XOR e suas implica√ß√µes para a evolu√ß√£o das Redes Neurais\n"
      ],
      "metadata": {
        "id": "S2-FF-sgj_HD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.1. Revis√£o do Problema L√≥gico XOR\n",
        "Tabela verdade do operador XOR\n",
        "\n",
        "Interpreta√ß√£o l√≥gica: sa√≠da 1 quando os valores s√£o diferentes\n",
        "\n",
        "Representa√ß√£o gr√°fica: pontos no plano cartesiano\n",
        "\n",
        "3.2. Constru√ß√£o do Dataset XOR\n",
        "Cria√ß√£o manual das entradas e sa√≠das\n",
        "\n",
        "Representa√ß√£o bin√°ria e visualiza√ß√£o em gr√°fico\n",
        "\n",
        "3.3. Testando o Perceptron no XOR\n",
        "Utiliza√ß√£o da mesma implementa√ß√£o do Perceptron simples\n",
        "\n",
        "Treinamento com os dados do XOR\n",
        "\n",
        "Resultados obtidos: falhas na classifica√ß√£o correta\n",
        "\n",
        "3.4. Visualiza√ß√£o da Falha do Perceptron\n",
        "Gr√°fico mostrando que os dados n√£o s√£o linearmente separ√°veis\n",
        "\n",
        "Tentativas frustradas de ajustar uma reta que separe os dados\n",
        "\n",
        "Ilustra√ß√£o clara da limita√ß√£o do modelo linear\n",
        "\n",
        "3.5. Entendendo a Limita√ß√£o Te√≥rica\n",
        "Conceito de linear separability\n",
        "\n",
        "Prova informal de que XOR n√£o pode ser separado com uma √∫nica reta\n",
        "\n",
        "Implica√ß√£o direta: Perceptron simples n√£o √© suficiente\n",
        "\n",
        "3.6. Implica√ß√µes Hist√≥ricas e Cient√≠ficas\n",
        "Impacto da cr√≠tica de Minsky & Papert (1969) sobre o Perceptron\n",
        "\n",
        "Queda do interesse em redes neurais na d√©cada de 1970 (inverno da IA)\n",
        "\n",
        "Redescoberta e evolu√ß√£o nos anos 1980 com o surgimento do Backpropagation\n",
        "\n",
        "3.7. Solu√ß√£o: Redes Neurais Multicamadas (MLP)\n",
        "Introdu√ß√£o ao conceito de camada oculta\n",
        "\n",
        "Como uma MLP pode resolver o XOR\n",
        "\n",
        "Antecipa√ß√£o do pr√≥ximo passo na evolu√ß√£o das RNAs\n",
        "\n",
        "3.8. Conclus√£o e Conex√£o com o Futuro\n",
        "O problema XOR como marco hist√≥rico e ponto de virada\n",
        "\n",
        "Import√¢ncia da limita√ß√£o para o avan√ßo cient√≠fico\n",
        "\n",
        "Encaminhamento para estudos sobre redes multicamadas e deep learning\n",
        "\n"
      ],
      "metadata": {
        "id": "LfKFyhsk47d_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ziEtDXZ_hGEm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Refer√™ncias\n",
        "MCCULLOCH, Warren S.; PITTS, Walter. A logical calculus of the ideas immanent in nervous activity. The Bulletin of Mathematical Biophysics, v. 5, p. 115‚Äì133, 1943. Reimpresso em: Bulletin of Mathematical Biology, v. 52, n. 1/2, p. 99‚Äì115, 1990.\n",
        "\n",
        "ROSENBLATT, Frank. The perceptron: a probabilistic model for information storage and organization in the brain. Psychological Review, v. 65, n. 6, p. 386‚Äì408, 1958.\n",
        "\n",
        "RUMELHART, David E.; HINTON, Geoffrey E.; MCCLELLAND, James L. Learning internal representations by error propagation. In: RUMELHART, D. E.; MCCLELLAND, J. L.; PDP Research Group (Ed.). Parallel Distributed Processing: Explorations in the Microstructure of Cognition. v. 1: Foundations. Cambridge, MA: MIT Press, 1986. p. 318‚Äì362.\n",
        "\n",
        "Du, K.-L.; Leung, C.-S.; Mow, W.H.; Swamy, M.N.S. Perceptron: Learning, Generalization, Model Selection, Fault Tolerance, and Role in the Deep Learning Era. Mathematics 2022, 10, 4730. https://doi.org/10.3390/math10244730"
      ],
      "metadata": {
        "id": "dHzyVon7r7Js"
      }
    }
  ]
}