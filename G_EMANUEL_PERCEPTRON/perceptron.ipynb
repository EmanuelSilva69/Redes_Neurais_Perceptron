{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Atividade de Redes Neurais de Emanuel Lopes Silva"
      ],
      "metadata": {
        "id": "co0N8ZRrkDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este notebook foi desenvolvido como um guia teórico e prático introdutório sobre Redes Neurais Artificiais (RNAs), com o objetivo de apresentar os conceitos fundamentais e exemplos de implementação em Python de forma clara e acessível.\n",
        "\n",
        "As Redes Neurais Artificiais são sistemas computacionais inspirados na estrutura e no funcionamento do cérebro humano. Elas utilizam unidades chamadas neurônios artificiais, organizadas em camadas e interconectadas por pesos sinápticos, os quais são ajustados automaticamente durante o aprendizado. Com isso, as RNAs são capazes de identificar padrões, realizar classificações e fazer previsões com base em dados, sendo amplamente utilizadas em diversas aplicações da inteligência artificial."
      ],
      "metadata": {
        "id": "Vao65IKjmWYc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#1. Introdução ao modelo de Perceptron\n",
        "\n"
      ],
      "metadata": {
        "id": "iFJOlLgLixWJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.1. O que é o Perceptron?\n",
        "\n",
        "###1.1.1 Definição e propósito\n",
        "\n",
        "O Perceptron é considerado o primeiro modelo computacional de rede neural com capacidade de aprendizado supervisionado, sendo formulado por Frank Rosenblatt, em 1958. Seu objetivo era simular o processo de reconhecimento de padrões por organismos biológicos, estabelecendo uma ligação entre a estrutura neural e o comportamento adaptativo. Segundo Rosenblatt, o Perceptron é um \"modelo probabilístico de organização e armazenamento de informação no cérebro\", cuja função central é aprender a associar padrões de entrada a respostas esperadas com base em exemplos fornecidos previamente (ROSENBLATT, 1958).\n",
        "\n",
        "Seu propósito pode ser simplificado nos seguintes pontos:\n",
        "- Simular o processo de **aprendizado supervisionado** inspirado no cérebro humano.\n",
        "- Estabelecer um **modelo matemático** capaz de reconhecer padrões a partir de exemplos.\n",
        "- Demonstrar que é possível implementar **funções de decisão lineares** com neurônios artificiais simples.\n",
        "- Investigar como a **informação é armazenada e organizada** em redes de conexões sinápticas.\n",
        "- Propor um sistema que aprende ajustando **pesos sinápticos** com base no erro da resposta.\n",
        "- Oferecer uma base teórica para a construção de redes neurais mais complexas no futuro.\n",
        "- Explorar o potencial de redes com **unidades simples interconectadas** para gerar comportamento inteligente.\n",
        "\n",
        "###1.1.2 Origem Histórica\n",
        "\n",
        "A origem do modelo de Perceptron remonta à tentativa de compreender os mecanismos de aprendizagem e reconhecimento presentes nos sistemas nervosos biológicos. O trabalho seminal de McCulloch e Pitts (1943) já propunha uma estrutura lógica para descrever a atividade neural por meio de funções booleanas, estabelecendo os fundamentos do que viria a ser conhecido como redes neurais artificiais. Eles demonstraram que \"a atividade de qualquer neurônio pode ser representada como uma proposição\" e que redes neurais poderiam, em princípio, implementar qualquer função lógica computável com base em conexões sinápticas e limiares de ativação​.\n",
        "\n",
        "Décadas depois, Frank Rosenblatt, em seu artigo de 1958, formalizou o modelo do Perceptron como uma máquina probabilística capaz de realizar tarefas de classificação por meio da aprendizagem dos pesos sinápticos. Segundo Rosenblatt, o Perceptron representa um \"sistema nervoso hipotético\" que armazena informação por meio de conexões modificáveis, sendo capaz de generalizar e reconhecer padrões com base em estímulos sensoriais​\n",
        ". Esse modelo é considerado a primeira implementação prática de uma rede neural com capacidade de aprendizado supervisionado.\n",
        "\n",
        "O entusiasmo inicial com o Perceptron, no entanto, foi confrontado com críticas importantes. O livro Perceptrons de Minsky e Papert (1969) demonstrou limitações fundamentais do modelo, especialmente sua incapacidade de resolver problemas não linearmente separáveis, como o XOR. Essa crítica desacelerou o avanço das redes neurais por quase duas décadas.\n",
        "\n",
        "Somente nos anos 1980, com o trabalho de Rumelhart, Hinton e McClelland, foi possível revitalizar o campo por meio do conceito de Parallel Distributed Processing (PDP). Esses pesquisadores defenderam que o processamento inteligente poderia emergir da interação paralela de unidades simples de processamento, reforçando o papel das redes distribuídas e do aprendizado supervisionado por propagação do erro como alternativa viável e biologicamente plausível para modelar a cognição humana​\n",
        ".\n",
        "\n",
        "###1.1.3 Importância na evolução das redes neurais\n",
        "O modelo do Perceptron teve um papel fundamental na história da Inteligência Artificial, sendo considerado o ponto de partida das redes neurais artificiais com capacidade de aprendizado. Proposto por Frank Rosenblatt em 1958, o Perceptron foi o primeiro sistema computacional que demonstrou ser possível ensinar uma máquina a reconhecer padrões por meio de ajustes iterativos em seus parâmetros internos, simulando o processo de aprendizagem observado em sistemas biológicos (ROSENBLATT, 1958). Seu funcionamento baseado na atualização de pesos sinápticos a partir do erro da saída contribuiu para consolidar o paradigma conexionista como alternativa aos modelos simbólicos dominantes na época.\n",
        "\n",
        "Ele foi essencial na evolução das redes neurais, pois:\n",
        "\n",
        "- Foi o primeiro modelo de rede neural com capacidade de aprendizado supervisionado.\n",
        "- Estabeleceu a ideia de ajuste de pesos sinápticos como mecanismo de aprendizagem.\n",
        "- Inspirou o desenvolvimento de redes neurais multicamadas (MLPs) e o uso da retropropagação.\n",
        "- Sua limitação com problemas como XOR motivou pesquisas sobre redes com maior capacidade expressiva.\n",
        "- Fundamentou a transição de modelos simbólicos para modelos conexionistas na IA.\n",
        "- Tornou-se referência obrigatória em estudos sobre o surgimento das redes neurais profundas.\n",
        "\n",
        "##1.2. Inspiração Biológica\n",
        "###1.2.1 Comparação entre neurônios biológicos e artificiais\n",
        "\n",
        "###1.2.2 Elementos equivalentes: dendritos, corpo celular, axônio, sinapse\n",
        "\n",
        "##1.3. Estrutura de um Perceptron\n",
        "Entradas e pesos\n",
        "\n",
        "Função de ativação (limiar)\n",
        "\n",
        "Saída binária (0 ou 1)\n",
        "\n",
        "Esquema visual do neurônio Perceptron\n",
        "\n",
        "##1.4. Funcionamento Básico\n",
        "Etapas do processo:\n",
        "\n",
        "Soma ponderada\n",
        "\n",
        "Aplicação da função de ativação\n",
        "\n",
        "Geração da saída\n",
        "\n",
        "##1.5. Fórmulas Matemáticas\n",
        "Equação da saída do Perceptron:\n",
        "\n",
        "𝑦\n",
        "=\n",
        "𝑓\n",
        "(\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "𝑤\n",
        "𝑖\n",
        "𝑥\n",
        "𝑖\n",
        "+\n",
        "𝑏\n",
        ")\n",
        "y=f(\n",
        "i=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " w\n",
        "i\n",
        "​\n",
        " x\n",
        "i\n",
        "​\n",
        " +b)\n",
        "Explicação de cada termo:\n",
        "\n",
        "𝑥\n",
        "𝑖\n",
        "x\n",
        "i\n",
        "​\n",
        " : entrada\n",
        "\n",
        "𝑤\n",
        "𝑖\n",
        "w\n",
        "i\n",
        "​\n",
        " : peso\n",
        "\n",
        "𝑏\n",
        "b: viés (bias)\n",
        "\n",
        "𝑓\n",
        "f: função de ativação (degrau de Heaviside)\n",
        "\n",
        "##1.6. Algoritmo de Aprendizado do Perceptron\n",
        "Lógica do treinamento supervisionado\n",
        "\n",
        "Atualização dos pesos:\n",
        "\n",
        "𝑤\n",
        "𝑖\n",
        "←\n",
        "𝑤\n",
        "𝑖\n",
        "+\n",
        "𝜂\n",
        "⋅\n",
        "(\n",
        "𝑦\n",
        "real\n",
        "−\n",
        "𝑦\n",
        "previsto\n",
        ")\n",
        "⋅\n",
        "𝑥\n",
        "𝑖\n",
        "w\n",
        "i\n",
        "​\n",
        " ←w\n",
        "i\n",
        "​\n",
        " +η⋅(y\n",
        "real\n",
        "​\n",
        " −y\n",
        "previsto\n",
        "​\n",
        " )⋅x\n",
        "i\n",
        "​\n",
        "\n",
        "Papel da taxa de aprendizado\n",
        "𝜂\n",
        "η\n",
        "\n",
        "##1.7. Convergência e Limitações\n",
        "Convergência garantida para problemas linearmente separáveis\n",
        "\n",
        "Limitações do modelo simples (não resolve o XOR)\n",
        "\n",
        "Introdução à motivação para redes multicamadas\n",
        "\n",
        "##1.8. Visualizações e Intuições Geométricas\n",
        "Fronteiras de decisão lineares\n",
        "\n",
        "Separação de classes no plano cartesiano\n",
        "\n",
        "##1.9. Variações e Extensões\n",
        "Perceptron com múltiplos neurônios (camada de saída com mais classes)\n",
        "\n",
        "Funções de ativação alternativas (ReLU, Sigmoid – apenas para contextualizar)\n",
        "\n"
      ],
      "metadata": {
        "id": "I1iWiR1NvqQ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Aplicação do Perceptron aos problemas lógicos OR e AND\n"
      ],
      "metadata": {
        "id": "BYlOWqBlj4A0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.1. Revisão dos Problemas Lógicos\n",
        "Tabelas verdade dos operadores lógicos AND e OR\n",
        "\n",
        "Representação como problemas de classificação binária\n",
        "\n",
        "2.2. Construção Manual dos Datasets\n",
        "Conjunto de dados com entrada binária (0 ou 1)\n",
        "\n",
        "Preparação dos pares (entrada, saída esperada)\n",
        "\n",
        "2.3. Implementação do Perceptron\n",
        "Código Python manual, com comentários linha a linha\n",
        "\n",
        "Inicialização de pesos e viés\n",
        "\n",
        "Função de ativação (degrau)\n",
        "\n",
        "Laço de treinamento com atualização dos pesos\n",
        "\n",
        "2.4. Processo de Treinamento\n",
        "Número de épocas\n",
        "\n",
        "Cálculo do erro\n",
        "\n",
        "Atualização iterativa dos pesos\n",
        "\n",
        "2.5. Avaliação do Modelo\n",
        "Previsão de saída para cada entrada\n",
        "\n",
        "Comparação com a saída esperada\n",
        "\n",
        "Impressão de resultados\n",
        "\n",
        "2.6. Visualização dos Resultados\n",
        "Gráficos de dispersão dos dados\n",
        "\n",
        "Exibição da fronteira de decisão\n",
        "\n",
        "Análise visual: separabilidade linear\n",
        "\n",
        "2.7. Análise da Fronteira de Decisão\n",
        "Interpretação geométrica dos pesos como coeficientes da reta\n",
        "\n",
        "Como a reta se ajusta para classificar corretamente os dados\n",
        "\n",
        "Demonstração com matplotlib (sem bibliotecas de IA)\n",
        "\n",
        "2.8. Discussão dos Resultados\n",
        "Por que o Perceptron funciona para OR e AND?\n",
        "\n",
        "Influência da taxa de aprendizado e número de épocas\n",
        "\n",
        "Limitações do modelo simples mesmo em problemas lineares (por ex., quando dados ruidosos são inseridos)"
      ],
      "metadata": {
        "id": "FOpC_xcOzAnA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Discussão do problema XOR e suas implicações para a evolução das Redes Neurais\n"
      ],
      "metadata": {
        "id": "S2-FF-sgj_HD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.1. Revisão do Problema Lógico XOR\n",
        "Tabela verdade do operador XOR\n",
        "\n",
        "Interpretação lógica: saída 1 quando os valores são diferentes\n",
        "\n",
        "Representação gráfica: pontos no plano cartesiano\n",
        "\n",
        "3.2. Construção do Dataset XOR\n",
        "Criação manual das entradas e saídas\n",
        "\n",
        "Representação binária e visualização em gráfico\n",
        "\n",
        "3.3. Testando o Perceptron no XOR\n",
        "Utilização da mesma implementação do Perceptron simples\n",
        "\n",
        "Treinamento com os dados do XOR\n",
        "\n",
        "Resultados obtidos: falhas na classificação correta\n",
        "\n",
        "3.4. Visualização da Falha do Perceptron\n",
        "Gráfico mostrando que os dados não são linearmente separáveis\n",
        "\n",
        "Tentativas frustradas de ajustar uma reta que separe os dados\n",
        "\n",
        "Ilustração clara da limitação do modelo linear\n",
        "\n",
        "3.5. Entendendo a Limitação Teórica\n",
        "Conceito de linear separability\n",
        "\n",
        "Prova informal de que XOR não pode ser separado com uma única reta\n",
        "\n",
        "Implicação direta: Perceptron simples não é suficiente\n",
        "\n",
        "3.6. Implicações Históricas e Científicas\n",
        "Impacto da crítica de Minsky & Papert (1969) sobre o Perceptron\n",
        "\n",
        "Queda do interesse em redes neurais na década de 1970 (inverno da IA)\n",
        "\n",
        "Redescoberta e evolução nos anos 1980 com o surgimento do Backpropagation\n",
        "\n",
        "3.7. Solução: Redes Neurais Multicamadas (MLP)\n",
        "Introdução ao conceito de camada oculta\n",
        "\n",
        "Como uma MLP pode resolver o XOR\n",
        "\n",
        "Antecipação do próximo passo na evolução das RNAs\n",
        "\n",
        "3.8. Conclusão e Conexão com o Futuro\n",
        "O problema XOR como marco histórico e ponto de virada\n",
        "\n",
        "Importância da limitação para o avanço científico\n",
        "\n",
        "Encaminhamento para estudos sobre redes multicamadas e deep learning\n",
        "\n"
      ],
      "metadata": {
        "id": "LfKFyhsk47d_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ziEtDXZ_hGEm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Referências\n",
        "MCCULLOCH, Warren S.; PITTS, Walter. A logical calculus of the ideas immanent in nervous activity. The Bulletin of Mathematical Biophysics, v. 5, p. 115–133, 1943. Reimpresso em: Bulletin of Mathematical Biology, v. 52, n. 1/2, p. 99–115, 1990.\n",
        "\n",
        "ROSENBLATT, Frank. The perceptron: a probabilistic model for information storage and organization in the brain. Psychological Review, v. 65, n. 6, p. 386–408, 1958.\n",
        "\n",
        "RUMELHART, David E.; HINTON, Geoffrey E.; MCCLELLAND, James L. Learning internal representations by error propagation. In: RUMELHART, D. E.; MCCLELLAND, J. L.; PDP Research Group (Ed.). Parallel Distributed Processing: Explorations in the Microstructure of Cognition. v. 1: Foundations. Cambridge, MA: MIT Press, 1986. p. 318–362.\n",
        "\n",
        "Du, K.-L.; Leung, C.-S.; Mow, W.H.; Swamy, M.N.S. Perceptron: Learning, Generalization, Model Selection, Fault Tolerance, and Role in the Deep Learning Era. Mathematics 2022, 10, 4730. https://doi.org/10.3390/math10244730"
      ],
      "metadata": {
        "id": "dHzyVon7r7Js"
      }
    }
  ]
}